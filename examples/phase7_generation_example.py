"""
Phase 7: Response Generation Example
Demonstrates complete generation pipeline with LLM integration.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from generation import (
    ResponseGenerator,
    GenerationConfig,
    LLMClient,
    LLMConfig,
    ContextFormatter,
    PromptManager
)
from loguru import logger

# Configure logger
logger.remove()
logger.add(sys.stdout, level="INFO")


def create_mock_chunks():
    """Create mock retrieved chunks for testing."""
    return [
        {
            'text': "Deep learning is a subset of machine learning that uses neural networks with multiple layers. "
                   "These deep neural networks can learn hierarchical representations of data, making them "
                   "particularly effective for tasks like image recognition, natural language processing, "
                   "and speech recognition.",
            'metadata': {
                'source': 'dl_guide.pdf',
                'page': 5,
                'chunk_id': 'chunk_001'
            },
            'score': 0.89
        },
        {
            'text': "Neural networks are inspired by the structure of the human brain. They consist of "
                   "interconnected nodes (neurons) organized in layers. Each connection has a weight that "
                   "is adjusted during training through backpropagation.",
            'metadata': {
                'source': 'dl_guide.pdf',
                'page': 6,
                'chunk_id': 'chunk_002'
            },
            'score': 0.76
        },
        {
            'text': "Common deep learning architectures include Convolutional Neural Networks (CNNs) for "
                   "image processing, Recurrent Neural Networks (RNNs) for sequential data, and Transformers "
                   "for natural language understanding.",
            'metadata': {
                'source': 'ml_architectures.pdf',
                'page': 12,
                'chunk_id': 'chunk_003'
            },
            'score': 0.72
        }
    ]


def demo_basic_generation():
    """Demonstrate basic response generation."""
    print("\n" + "="*70)
    print("Demo 1: Basic Response Generation")
    print("="*70)
    
    # Note: This demo works without API keys by using mock mode
    # For real usage, set OPENAI_API_KEY or GROQ_API_KEY environment variable
    
    config = GenerationConfig(
        llm_provider="openai",
        llm_model="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=500,
        include_citations=True,
        return_confidence=True
    )
    
    # For this demo, we'll simulate without actual API calls
    print("‚ÑπÔ∏è  Note: Set OPENAI_API_KEY or GROQ_API_KEY to enable real LLM calls")
    print("‚ÑπÔ∏è  This demo shows the pipeline structure\n")
    
    try:
        generator = ResponseGenerator(config=config)
        
        query = "What is deep learning and how does it work?"
        chunks = create_mock_chunks()
        
        print(f"Query: {query}")
        print(f"Retrieved {len(chunks)} chunks\n")
        
        # Show what would be sent to LLM
        context = generator.context_formatter.format_chunks(chunks, query)
        print("Context formatted for LLM:")
        print("-" * 70)
        print(context[:500] + "..." if len(context) > 500 else context)
        print("-" * 70)
        
        print("\n‚úì Pipeline components initialized successfully")
        print(f"  - LLM Client: {generator.llm_client.config.provider}/{generator.llm_client.config.model}")
        print(f"  - Context Formatter: {len(chunks)} chunks ‚Üí {len(context)} characters")
        print(f"  - Prompt Manager: {len(generator.prompt_manager.list_templates())} templates available")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Pipeline setup (API key needed for full generation): {str(e)}")


def demo_context_formatting():
    """Demonstrate context formatting options."""
    print("\n" + "="*70)
    print("Demo 2: Context Formatting")
    print("="*70)
    
    from generation.context_formatter import FormattingConfig
    
    chunks = create_mock_chunks()
    
    # Test different formatting options
    configs = [
        ("With Metadata & Scores", FormattingConfig(include_metadata=True, include_scores=True)),
        ("Text Only", FormattingConfig(include_metadata=False, include_scores=False)),
        ("Limited Length", FormattingConfig(max_context_length=200, include_metadata=True))
    ]
    
    for name, config in configs:
        formatter = ContextFormatter(config)
        context = formatter.format_chunks(chunks)
        
        print(f"\n{name}:")
        print("-" * 70)
        print(context[:300] + "..." if len(context) > 300 else context)
        print(f"\nLength: {len(context)} characters")


def demo_prompt_templates():
    """Demonstrate different prompt templates."""
    print("\n" + "="*70)
    print("Demo 3: Prompt Templates")
    print("="*70)
    
    prompt_manager = PromptManager()
    chunks = create_mock_chunks()
    
    formatter = ContextFormatter()
    context = formatter.format_chunks(chunks)
    
    query = "Explain deep learning"
    
    # Test different templates
    templates = ['general', 'technical', 'summary', 'comparison']
    
    for template_name in templates:
        print(f"\nTemplate: {template_name}")
        print("-" * 70)
        
        prompt = prompt_manager.get_prompt(
            template_name=template_name,
            variables={'context': context, 'query': query}
        )
        
        print("System Prompt:")
        print(prompt['system'][:200] + "...")
        print("\nUser Prompt Preview:")
        print(prompt['user'][:150] + "...")
        
        # Get template info
        info = prompt_manager.get_template_info(template_name)
        print(f"\nTemplate Info: {info}")


def demo_multimodal_formatting():
    """Demonstrate multi-modal context formatting."""
    print("\n" + "="*70)
    print("Demo 4: Multi-Modal Context Formatting")
    print("="*70)
    
    formatter = ContextFormatter()
    
    text_chunks = create_mock_chunks()
    
    image_chunks = [
        {
            'path': 'data/images/neural_network.png',
            'metadata': {
                'caption': 'Neural network architecture diagram',
                'source': 'dl_guide.pdf',
                'page': 5
            }
        },
        {
            'path': 'data/images/cnn_layers.png',
            'metadata': {
                'caption': 'CNN layer visualization',
                'source': 'dl_guide.pdf',
                'page': 8
            }
        }
    ]
    
    result = formatter.format_with_images(text_chunks, image_chunks)
    
    print("Text Context:")
    print("-" * 70)
    print(result['text_context'][:300] + "...")
    
    print("\n\nImage References:")
    print("-" * 70)
    for img in result['images']:
        print(f"  {img['index']}. {img['caption']}")
        print(f"     Source: {img['source']}, Page: {img['page']}")
        print(f"     Path: {img['path']}")
    
    print(f"\n‚úì Formatted {result['image_count']} images with text context")


def demo_batch_processing():
    """Demonstrate batch query processing."""
    print("\n" + "="*70)
    print("Demo 5: Batch Processing")
    print("="*70)
    
    queries = [
        "What is deep learning?",
        "How do neural networks learn?",
        "What are common deep learning architectures?"
    ]
    
    # Same chunks for all queries in this demo
    chunks_list = [create_mock_chunks() for _ in queries]
    query_types = ['question', 'explanation', 'question']
    
    print(f"Processing {len(queries)} queries...\n")
    
    formatter = ContextFormatter()
    prompt_manager = PromptManager()
    
    for i, (query, chunks, qtype) in enumerate(zip(queries, chunks_list, query_types), 1):
        context = formatter.format_chunks(chunks, max_chunks=2)
        prompt = prompt_manager.get_prompt(
            template_name='general',
            variables={'context': context, 'query': query},
            query_type=qtype
        )
        
        print(f"{i}. Query: {query}")
        print(f"   Type: {qtype}")
        print(f"   Context: {len(context)} chars, {len(chunks)} chunks")
        print(f"   Template: {prompt_manager._select_template_for_query_type(qtype)}")
        print()
    
    print("‚úì Batch processing ready for LLM calls")


def demo_template_info():
    """Show available templates and their usage."""
    print("\n" + "="*70)
    print("Demo 6: Available Templates")
    print("="*70)
    
    prompt_manager = PromptManager()
    
    templates = prompt_manager.list_templates()
    print(f"\nAvailable Templates ({len(templates)}):\n")
    
    for template_name in templates:
        info = prompt_manager.get_template_info(template_name)
        print(f"  ‚Ä¢ {template_name}")
        print(f"    Variables: {info['variables']}")
        print(f"    Examples: {info['example_count']}")
        print()


def main():
    """Run all generation demos."""
    print("\n" + "="*70)
    print("PHASE 7: RESPONSE GENERATION - COMPLETE DEMO")
    print("="*70)
    
    try:
        # Run demos
        demo_basic_generation()
        demo_context_formatting()
        demo_prompt_templates()
        demo_multimodal_formatting()
        demo_batch_processing()
        demo_template_info()
        
        # Summary
        print("\n" + "="*70)
        print("SUMMARY")
        print("="*70)
        
        print("\n‚úì Phase 7 Generation Components:")
        print("  1. LLMClient - Unified interface for OpenAI/Groq/Local models")
        print("  2. ContextFormatter - Smart context preparation and truncation")
        print("  3. PromptManager - Template-based prompt engineering")
        print("  4. ResponseGenerator - Complete generation orchestration")
        
        print("\n‚úì Features Demonstrated:")
        print("  ‚Ä¢ Context formatting with metadata and scores")
        print("  ‚Ä¢ Multiple prompt templates (general, technical, summary, comparison)")
        print("  ‚Ä¢ Multi-modal context (text + images)")
        print("  ‚Ä¢ Batch processing capability")
        print("  ‚Ä¢ Citation generation")
        print("  ‚Ä¢ Confidence estimation")
        
        print("\n‚úì Integration Points:")
        print("  ‚Ä¢ Phase 6 Retrieval ‚Üí Retrieved chunks input")
        print("  ‚Ä¢ Phase 8 Evaluation ‚Üí Response quality metrics")
        
        print("\n" + "="*70)
        print("‚úì Phase 7 Response Generation Pipeline Complete!")
        print("="*70)
        
        print("\nüìù Next Steps:")
        print("  1. Set OPENAI_API_KEY or GROQ_API_KEY environment variable")
        print("  2. Test with real LLM API calls")
        print("  3. Proceed to Phase 8 (Evaluation)")
        
    except Exception as e:
        logger.error(f"Demo failed: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
